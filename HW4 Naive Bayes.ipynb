{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd50bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa6d3be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    X = []  # List of documents\n",
    "    y = []  # List of labels (e for English, j for Japanese, s for Spanish)\n",
    "    \n",
    "    for c in ['e', 'j', 's']:\n",
    "        for i in range(10):\n",
    "            filename = os.path.join(data_dir, f'{c}{i}.txt')\n",
    "            with open(filename, 'r', encoding='utf-8') as file:\n",
    "                text = file.read().replace('\\n', '')\n",
    "                X.append(text)\n",
    "                y.append(c)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def calculate_prior_probabilities(y):\n",
    "    num_classes = len(set(y))\n",
    "    prior_probabilities = {}\n",
    "    \n",
    "    for c in ['e', 'j', 's']:\n",
    "        c_count = y.count(c)\n",
    "        prior_probabilities[c] = (c_count + 0.5) / (len(y) + num_classes * 0.5)\n",
    "    return prior_probabilities\n",
    "\n",
    "# Define the character vocabulary (a to z and space)\n",
    "vocabulary = 'abcdefghijklmnopqrstuvwxyz '\n",
    "\n",
    "def calculate_class_conditional_probability1(X, y, vocabulary, c, smoothing_param=0.5):\n",
    "    char_count_per_class = Counter()\n",
    "    total_char_count_per_class = 0\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        if y[i] == c:\n",
    "            # updates the count of characters in the char_count object with the characters found in the document\n",
    "            char_count_per_class.update(X[i])\n",
    "            total_char_count_per_class += len(X[i])\n",
    "            \n",
    "    theta_ie = {}\n",
    "    for char in vocabulary:\n",
    "        char_count = char_count_per_class[char]\n",
    "        prob = (char_count + smoothing_param) / (total_char_count_per_class + len(vocabulary) * smoothing_param)\n",
    "        theta_ie[char] = prob\n",
    "    return theta_ie         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70dae4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Probabilities: {'e': 0.3333333333333333, 'j': 0.3333333333333333, 's': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'languageID'\n",
    "X, y = load_data(data_dir)\n",
    "\n",
    "#Q1\n",
    "prior_probabilities = calculate_prior_probabilities(y)\n",
    "print(\"Prior Probabilities:\", prior_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "559bf1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Conditional Probability for English:\n",
      "0.0602 \\\\\n",
      "0.0111 \\\\\n",
      "0.0215 \\\\\n",
      "0.0220 \\\\\n",
      "0.1054 \\\\\n",
      "0.0189 \\\\\n",
      "0.0175 \\\\\n",
      "0.0472 \\\\\n",
      "0.0554 \\\\\n",
      "0.0014 \\\\\n",
      "0.0037 \\\\\n",
      "0.0290 \\\\\n",
      "0.0205 \\\\\n",
      "0.0579 \\\\\n",
      "0.0645 \\\\\n",
      "0.0168 \\\\\n",
      "0.0006 \\\\\n",
      "0.0538 \\\\\n",
      "0.0662 \\\\\n",
      "0.0801 \\\\\n",
      "0.0267 \\\\\n",
      "0.0093 \\\\\n",
      "0.0155 \\\\\n",
      "0.0012 \\\\\n",
      "0.0138 \\\\\n",
      "0.0006 \\\\\n",
      "0.1792 \\\\\n"
     ]
    }
   ],
   "source": [
    "theta_e = calculate_class_conditional_probability1(X, y, vocabulary, 'e')\n",
    "print(\"Class Conditional Probability for English:\")\n",
    "for char, prob in theta_e.items():\n",
    "    prob = \"{:.4f}\".format(prob)\n",
    "    #print(f\"{char}: {prob}\")\n",
    "    print(f\"{prob} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc2f3a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Conditional Probability for Japanese:\n",
      "0.1318 \\\\\n",
      "0.0109 \\\\\n",
      "0.0055 \\\\\n",
      "0.0172 \\\\\n",
      "0.0602 \\\\\n",
      "0.0039 \\\\\n",
      "0.0140 \\\\\n",
      "0.0318 \\\\\n",
      "0.0970 \\\\\n",
      "0.0023 \\\\\n",
      "0.0574 \\\\\n",
      "0.0014 \\\\\n",
      "0.0398 \\\\\n",
      "0.0567 \\\\\n",
      "0.0912 \\\\\n",
      "0.0009 \\\\\n",
      "0.0001 \\\\\n",
      "0.0428 \\\\\n",
      "0.0422 \\\\\n",
      "0.0570 \\\\\n",
      "0.0706 \\\\\n",
      "0.0002 \\\\\n",
      "0.0197 \\\\\n",
      "0.0000 \\\\\n",
      "0.0142 \\\\\n",
      "0.0077 \\\\\n",
      "0.1234 \\\\\n"
     ]
    }
   ],
   "source": [
    "theta_j = calculate_class_conditional_probability1(X, y, vocabulary, 'j')\n",
    "print(\"Class Conditional Probability for Japanese:\")\n",
    "for char, prob in theta_j.items():\n",
    "    prob = \"{:.4f}\".format(prob)\n",
    "    #print(f\"{char}: {prob}\")\n",
    "    print(f\"{prob} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ef897f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Conditional Probability for Spanish:\n",
      "0.1046 \\\\\n",
      "0.0082 \\\\\n",
      "0.0375 \\\\\n",
      "0.0397 \\\\\n",
      "0.1138 \\\\\n",
      "0.0086 \\\\\n",
      "0.0072 \\\\\n",
      "0.0045 \\\\\n",
      "0.0499 \\\\\n",
      "0.0066 \\\\\n",
      "0.0003 \\\\\n",
      "0.0529 \\\\\n",
      "0.0258 \\\\\n",
      "0.0542 \\\\\n",
      "0.0725 \\\\\n",
      "0.0243 \\\\\n",
      "0.0077 \\\\\n",
      "0.0593 \\\\\n",
      "0.0658 \\\\\n",
      "0.0356 \\\\\n",
      "0.0337 \\\\\n",
      "0.0059 \\\\\n",
      "0.0001 \\\\\n",
      "0.0025 \\\\\n",
      "0.0079 \\\\\n",
      "0.0027 \\\\\n",
      "0.1683 \\\\\n"
     ]
    }
   ],
   "source": [
    "theta_s = calculate_class_conditional_probability1(X, y, vocabulary, 's')\n",
    "print(\"Class Conditional Probability for Spanish:\")\n",
    "for char, prob in theta_s.items():\n",
    "    prob = \"{:.4f}\".format(prob)\n",
    "    #print(f\"{char}: {prob}\")\n",
    "    print(f\"{prob} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4a0b77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except when the winds rise to a high speed we seem to live in a very tranquil world at night when the glare of the sun passes out of our atmosphere the stars and planets seem to move across the heavens with a stately and solemn slowness it was one of the first discoveries of modern astronomy that this movement is only apparent the apparent creeping of the stars across the heavens at night is accounted for by the fact that the earth turns upon its axis once in every twentyfour hours when we remember the size of the earth we see that this implies a prodigious speedfig the milky wayit had remained unchanged since noon of the previous daya long low quietlooking cloud not very dense or brilliant or in any way remarkable except for its size at  pm the professor left the spectroscope for a short time and on returning half an hour later to his observations he was astonished to find the gigantic sun flame shattered to pieces the solar atmosphere was filled with flying debris and some of these portions reached a height of  miles above the solar surface moving with a velocity which even at the distance of  miles was almost perceptible to the eye these fragments doubled their height in ten minutes on january   another distinguished solar observer the late professor tacchini of rome observed one of the greatest prominences ever seen by man its height was no less than  mileseighteen times the diameter of the earth another mighty flame was so vast that supposing the eight large planets of the solar system ranged one on top of the other the prominence would still tower above themhere let us return to and see what more we know about the photospherethe suns surface it is from the photosphere that we have gained most of our knowledge of the composition of the sun which is believed not to be a solid body examination of the photosphere shows that the outer surface is never at rest small bright cloudlets come and go in rapid succession giving the surface through contrasts in luminosity a granular appearance of course to be visible at all at  miles the cloudlets cannot be small they imply enormous activity in the photosphere if we might speak picturesquely the suns surface resembles a boiling ocean of whitehot metal vapours we have today a wonderful instrument which will be described later which dilutes as it were the general glare of the sun and enables us to observe these fiery eruptions at any hour the oceans of redhot gas and whitehot metal vapour at the suns surface are constantly driven by great storms some unimaginable energy streams out from the body or muscles of the sun and blows its outer layers into gigantic shreds as it werepg the great sunspot of july  showing the rings mighty swarms of meteorites drawing by prof lowell made january  \n",
      "164 \\\\\n",
      "32 \\\\\n",
      "53 \\\\\n",
      "57 \\\\\n",
      "311 \\\\\n",
      "55 \\\\\n",
      "51 \\\\\n",
      "140 \\\\\n",
      "140 \\\\\n",
      "3 \\\\\n",
      "6 \\\\\n",
      "85 \\\\\n",
      "64 \\\\\n",
      "139 \\\\\n",
      "182 \\\\\n",
      "53 \\\\\n",
      "3 \\\\\n",
      "141 \\\\\n",
      "186 \\\\\n",
      "225 \\\\\n",
      "65 \\\\\n",
      "31 \\\\\n",
      "47 \\\\\n",
      "4 \\\\\n",
      "38 \\\\\n",
      "2 \\\\\n",
      "498 \\\\\n"
     ]
    }
   ],
   "source": [
    "#Q4\n",
    "def calculate_bag_of_characters(X, y, vocabulary):\n",
    "    char_count = Counter()\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        char_count.update(X[i])\n",
    "\n",
    "    count = {}\n",
    "    for char in vocabulary:\n",
    "        count[char] = char_count[char]\n",
    "        #print(count[char])\n",
    "\n",
    "    return count\n",
    "\n",
    "X_test = []  # List of documents\n",
    "y_test = []\n",
    "filename = os.path.join(data_dir, 'e10.txt')\n",
    "with open(filename, 'r', encoding='utf-8') as file:\n",
    "    text = file.read().replace('\\n', '')\n",
    "    print(text)\n",
    "    X_test.append(text)\n",
    "    y_test.append('e')\n",
    "    \n",
    "bag_of_chars = calculate_bag_of_characters(X_test, y_test, vocabulary)\n",
    "for char, count in bag_of_chars.items():\n",
    "    print(f\"{count} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df226f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0.0601685114819098, 'b': 0.011134974392863043, 'c': 0.021509995043779945, 'd': 0.021972575582355856, 'e': 0.1053692383941847, 'f': 0.018932760614571286, 'g': 0.017478936064761277, 'h': 0.047216256401784236, 'i': 0.055410540227986124, 'j': 0.001420783082768875, 'k': 0.0037336857756484387, 'l': 0.028977366595076822, 'm': 0.020518751032545846, 'n': 0.057921691723112505, 'o': 0.06446390219725756, 'p': 0.01675202378985627, 'q': 0.0005617049396993227, 'r': 0.053824549810011564, 's': 0.06618205848339666, 't': 0.08012555757475633, 'u': 0.026664463902197257, 'v': 0.009284652238559392, 'w': 0.015496448042293078, 'x': 0.001156451346439782, 'y': 0.013844374690236246, 'z': 0.0006277878737815959, ' ': 0.1792499586981662}\n",
      "-------------------------------------------------------------\n",
      "{'a': 0.1317656102589189, 'b': 0.010866906600510151, 'c': 0.005485866033054963, 'd': 0.01722631818022992, 'e': 0.06020475907613823, 'f': 0.003878542227191726, 'g': 0.014011670568503443, 'h': 0.03176211607673224, 'i': 0.09703343932352633, 'j': 0.0023411020650616725, 'k': 0.05740941332681086, 'l': 0.001432614696530277, 'm': 0.03979873510604843, 'n': 0.05671057688947902, 'o': 0.09116321324993885, 'p': 0.0008735455466648031, 'q': 0.00010482546559977637, 'r': 0.04280373178657535, 's': 0.0421747789929767, 't': 0.056990111464411755, 'u': 0.07061742199238269, 'v': 0.0002445927530661449, 'w': 0.01974212935462455, 'x': 3.4941821866592126e-05, 'y': 0.01415143785596981, 'z': 0.00772214263251686, ' ': 0.12344945665466997}\n",
      "-------------------------------------------------------------\n",
      "{'a': 0.10456045141993771, 'b': 0.008232863618143134, 'c': 0.03752582405722919, 'd': 0.039745922111559924, 'e': 0.1138108599796491, 'f': 0.00860287996053159, 'g': 0.0071844839813758445, 'h': 0.0045327001942585795, 'i': 0.049859702136844375, 'j': 0.006629459467793161, 'k': 0.0002775122567913416, 'l': 0.052943171656748174, 'm': 0.02580863988159477, 'n': 0.054176559464709693, 'o': 0.07249236841293824, 'p': 0.02426690512164287, 'q': 0.007677839104560451, 'r': 0.05929511886774999, 's': 0.06577040485954797, 't': 0.03561407295488884, 'u': 0.03370232185254849, 'v': 0.00588942678301625, 'w': 9.250408559711388e-05, 'x': 0.0024976103111220747, 'y': 0.007862847275754679, 'z': 0.0026826184823163022, ' ': 0.16826493170115014}\n",
      "-------------------------------------------------------------\n",
      "-7841.865447060571\n",
      "-8771.433079075021\n",
      "-8467.282044010564\n"
     ]
    }
   ],
   "source": [
    "#Q5: test data e10.txt\n",
    "\n",
    "# Calculate p(x | y) for a document x and language y\n",
    "def calculate_log_likelihood(document, theta_y):\n",
    "    log_prob = 0\n",
    "    for char in document:\n",
    "        if char in theta_y:\n",
    "            log_prob += np.log(theta_y[char])\n",
    "            \n",
    "    return log_prob\n",
    "         \n",
    "\n",
    "print(theta_e)\n",
    "print('-------------------------------------------------------------')\n",
    "print(theta_j)\n",
    "print('-------------------------------------------------------------')\n",
    "print(theta_s)\n",
    "print('-------------------------------------------------------------')\n",
    "\n",
    "document = X_test[0]\n",
    "lang = y_test[0]\n",
    "\n",
    "p_x_given_e = calculate_log_likelihood(document, theta_e)\n",
    "p_x_given_j = calculate_log_likelihood(document, theta_j)\n",
    "p_x_given_s = calculate_log_likelihood(document, theta_s)\n",
    "\n",
    "print(p_x_given_e)\n",
    "print(p_x_given_j)\n",
    "print(p_x_given_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee091fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2c7361c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Posterior Probability (y = e | x): -7842.964059349239\n",
      "Log Posterior Probability (y = j | x): -8772.531691363689\n",
      "Log Posterior Probability (y = s | x): -8468.380656299232\n",
      "Predicted Language: y = e\n"
     ]
    }
   ],
   "source": [
    "#Q6\n",
    "log_prior_e = np.log(prior_probabilities['e'])\n",
    "log_prior_j = np.log(prior_probabilities['j'])\n",
    "log_prior_s = np.log(prior_probabilities['s'])\n",
    "\n",
    "log_posterior_e = p_x_given_e + log_prior_e\n",
    "log_posterior_j = p_x_given_j + log_prior_j\n",
    "log_posterior_s = p_x_given_s + log_prior_s\n",
    "\n",
    "log_posterior = np.array([log_posterior_e, log_posterior_j, log_posterior_s])\n",
    "\n",
    "print(\"Log Posterior Probability (y = e | x):\", log_posterior_e)\n",
    "print(\"Log Posterior Probability (y = j | x):\", log_posterior_j)\n",
    "print(\"Log Posterior Probability (y = s | x):\", log_posterior_s)\n",
    "\n",
    "# Predict the class label\n",
    "predicted_class = np.argmax(log_posterior)\n",
    "predicted_language = ['e', 'j', 's'][predicted_class]\n",
    "print(\"Predicted Language: y = {}\".format(predicted_language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51267a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  0 10]]\n"
     ]
    }
   ],
   "source": [
    "#Q7\n",
    "def load_test_data(data_dir):\n",
    "    X = []  # List of documents\n",
    "    y = []  # List of labels (e for English, j for Japanese, s for Spanish)\n",
    "    \n",
    "    for c in ['e', 'j', 's']:\n",
    "        for i in range(10, 20):\n",
    "            filename = os.path.join(data_dir, f'{c}{i}.txt')\n",
    "            with open(filename, 'r', encoding='utf-8') as file:\n",
    "                text = file.read().replace('\\n', '')\n",
    "                X.append(text)\n",
    "                y.append(c)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def confusion_matrix(X, y, theta_e, theta_j, theta_s, prior_probabilities):\n",
    "    num_classes = len(prior_probabilities)\n",
    "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        true_language = y[i]\n",
    "        posterior_probabilities = {\n",
    "            'e': np.log(prior_probabilities['e']) + calculate_log_likelihood(X[i], theta_e),\n",
    "            'j': np.log(prior_probabilities['j']) + calculate_log_likelihood(X[i], theta_j),\n",
    "            's': np.log(prior_probabilities['s']) + calculate_log_likelihood(X[i], theta_s),\n",
    "        }\n",
    "        \n",
    "        predicted_language = max(posterior_probabilities, key=posterior_probabilities.get)\n",
    "        \n",
    "        true_class = 'ejs'.index(true_language)\n",
    "        predicted_class = 'ejs'.index(predicted_language)\n",
    "        confusion_matrix[true_class][predicted_class] += 1\n",
    "        \n",
    "    return confusion_matrix\n",
    "\n",
    "        \n",
    "\n",
    "X_test2, y_test2 = load_test_data(data_dir)\n",
    "confusion_matrix = confusion_matrix(X_test2, y_test2, theta_e, theta_j, theta_s, prior_probabilities)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc623b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
